{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU0fkDiKCaj0"
      },
      "source": [
        "#Watson submission EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqRJTlKlE6rK"
      },
      "outputs": [],
      "source": [
        "#Review: building a Pytorch BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKIa81tfF-27",
        "outputId": "9ee2241f-a20f-4655-ba90-4c2c3ceb5aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 15.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 68.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 82.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 15.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.2-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 15.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (4.12.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (1.1.3)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 79.0 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.4.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 90.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=d6806ff32a4bf91ee966d57a0b0f12055aa71aeb04a2b02f555543e394560069\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.2 pbr-5.10.0 pyperclip-1.8.2 stevedore-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blo410DBiajN",
        "outputId": "b9da807d-c85b-44af-89bf-133df7e248e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Oct  3 23:55:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZAuAjtcBwdd",
        "outputId": "72567b32-6c10-4e66-b22e-2236e252e0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers.__version__: 4.22.2\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import transformers\n",
        "print(f\"transformers.__version__: {transformers.__version__}\")\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "class cfg:\n",
        "\n",
        "  #Data\n",
        "  data_loc = 'PATH TO TRAINING DATA HERE'\n",
        "  fold_df_loc = '/content/data_fold.csv'\n",
        "  n_folds = 5\n",
        "\n",
        "  #Model name\n",
        "  model_name = 'joeddav/xlm-roberta-large-xnli'\n",
        "  \n",
        "  gradient_checkpointing = False\n",
        "  gradient_accumulation_steps = 1\n",
        "  \n",
        "  #To use f16?\n",
        "  apex = False\n",
        "  \n",
        "  #Define parameters\n",
        "  batch_size = 8\n",
        "  max_epochs = 3\n",
        "  learning_rate = 5e-6\n",
        "\n",
        "  #Early stopping\n",
        "  patience = 1\n",
        "  max_len = 100\n",
        "\n",
        "  #Try out another loss:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  #Scheduler\n",
        "  scheduler = 'linear'\n",
        "  num_cycles = 0.5\n",
        "  num_warmup_steps = 0\n",
        "\n",
        "  #dropout\n",
        "  hidden_dropout = 0.1\n",
        "  hidden_dropout_prob = 0.1\n",
        "  attention_dropout = 0.1\n",
        "  attention_dropout_prob = 0.1\n",
        "\n",
        "  #last layer id\n",
        "  last_layer_id = -1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqDt2WgqB0-e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "df = pd.read_csv(cfg.data_loc)\n",
        "\n",
        "df.loc[:, 'kfold'] = -1\n",
        "df.sample(frac=1, random_state=23).reset_index(drop=True)\n",
        "labels = df.label.values\n",
        "\n",
        "kfolds = KFold(n_splits = cfg.n_folds)\n",
        "\n",
        "for fold, (trn,val) in enumerate(kfolds.split(X=df, y=labels)):\n",
        "  df.loc[val, 'kfold'] = fold\n",
        "\n",
        "cfg.train_len = len(df[df.kfold != 0])\n",
        "\n",
        "df.to_csv(\"data_fold.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMfsANo8KUtk"
      },
      "source": [
        "#DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tivblPnmFMHC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class WatsonDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer, max_len, inference=False):\n",
        "    \n",
        "    self.len = len(data)\n",
        "    self.max_len = max_len\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "    self.inference = inference\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    #Store premise and hypothesis\n",
        "    premis = self.data.premise[idx]\n",
        "    hypoth = self.data.hypothesis[idx]\n",
        "\n",
        "    #Combine into a single text string\n",
        "    txt = premis + ' [SEP] ' + hypoth\n",
        "\n",
        "    #Store label if not inference\n",
        "    if self.inference == False:\n",
        "      label = self.data.label[idx]\n",
        "\n",
        "    #Encode with tokenizer\n",
        "    encoding = self.tokenizer(txt, padding='max_length',\n",
        "                              truncation=True, \n",
        "                              max_length=self.max_len)\n",
        "    \n",
        "    #Convert to tensor\n",
        "    item = {key:torch.as_tensor(val) for key,val in encoding.items()}\n",
        "    \n",
        "    if self.inference == False:\n",
        "      item['label'] = torch.as_tensor(label)\n",
        "    \n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-A3GzRXKPu0"
      },
      "source": [
        "# The model/Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DiAQ-DxzZ8V"
      },
      "outputs": [],
      "source": [
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "    \n",
        "\n",
        "class PooledCustomModel(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        if config_path is None:\n",
        "            self.config = AutoConfig.from_pretrained(cfg.model_name, output_hidden_states=True)\n",
        "            self.config.hidden_dropout = self.cfg.hidden_dropout\n",
        "            self.config.hidden_dropout_prob = self.cfg.hidden_dropout_prob\n",
        "            self.config.attention_dropout = self.cfg.attention_dropout\n",
        "            self.config.attention_probs_dropout_prob = self.cfg.attention_dropout_prob\n",
        "            self.config.layer_norm_eps = self.cfg.layer_norm_eps\n",
        "        else:\n",
        "            self.config = torch.load(config_path)\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(cfg.model_name, config=self.config)\n",
        "        else:\n",
        "            self.model = AutoModel(self.config)\n",
        "        if self.cfg.gradient_checkpointing:\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "        self.pool = MeanPooling()\n",
        "        self.fc = nn.Linear(self.config.hidden_size, 3)\n",
        "        self._init_weights(self.fc)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        last_hidden_states = outputs[2][self.cfg.last_layer_id]\n",
        "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
        "        return feature\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        feature = self.feature(inputs)\n",
        "        output = self.fc(feature)\n",
        "        return output\n",
        "\n",
        "class Learning: #Integrate option of changing loss\n",
        "  def __init__(self, model, device, optimizer, cfg):\n",
        "    self.model = model\n",
        "    self.device = device\n",
        "    self.optimizer = optimizer\n",
        "    self.num_train_steps = cfg.train_len / cfg.batch_size * cfg.max_epochs\n",
        "\n",
        "  def loss_fn(self, output, label):\n",
        "    loss = nn.CrossEntropyLoss().to(self.device)\n",
        "    return loss(output, label)\n",
        "\n",
        "  def get_scheduler(self, cfg):\n",
        "    if cfg.scheduler == 'linear':\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=self.num_train_steps\n",
        "        )\n",
        "    elif cfg.scheduler == 'cosine':\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            self.optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=self.num_train_steps, num_cycles=cfg.num_cycles\n",
        "        )\n",
        "    return scheduler\n",
        "\n",
        "  def train_fn(self, dataloader):\n",
        "\n",
        "    self.model.train()\n",
        "    scheduler = self.get_scheduler(cfg)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.apex)\n",
        "\n",
        "    #tracking\n",
        "    step_loss = 0\n",
        "    training_steps = 0\n",
        "    pred_list, lab_list = [], []\n",
        "\n",
        "    for data in dataloader:\n",
        "\n",
        "      #To device\n",
        "      input_ids = data['input_ids'].to(self.device)\n",
        "      masks = data['attention_mask'].to(self.device)\n",
        "      labels = data['label'].to(self.device)\n",
        "\n",
        "      loss_fn = self.loss_fn\n",
        "\n",
        "      #Calc logits and loss\n",
        "      with torch.cuda.amp.autocast(enabled=cfg.apex):\n",
        "        logits = self.model({'input_ids':input_ids, 'attention_mask':masks})\n",
        "        loss = loss_fn(logits, labels)\n",
        "      \n",
        "      #Record step_loss and loss\n",
        "      step_loss += loss.item()\n",
        "      training_steps += 1\n",
        "      if training_steps % 200 == 0:\n",
        "        print(f\"Training loss after {training_steps} training steps: {step_loss/(training_steps*cfg.batch_size)}\")\n",
        "\n",
        "      #Flatten/take argmax of logits\n",
        "      flatten_labs = labels.view(-1).cpu().numpy()\n",
        "      training_logits = logits.view(-1, 3)\n",
        "      predictions = torch.argmax(training_logits, axis=1).cpu().numpy()\n",
        "\n",
        "      #For accuracy calc\n",
        "      pred_list.extend(predictions)\n",
        "      lab_list.extend(flatten_labs)\n",
        "\n",
        "      #gradient clipping\n",
        "      torch.nn.utils.clip_grad_norm_(parameters=self.model.parameters(), max_norm=10)\n",
        "\n",
        "      #Backward/optimizer/scheduler\n",
        "      self.optimizer.zero_grad()\n",
        "      \n",
        "      if cfg.apex == True:\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(self.optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "      \n",
        "      else:\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    #Calc acc\n",
        "    train_acc = accuracy_score(lab_list, pred_list)\n",
        "    train_loss = step_loss/training_steps\n",
        "    return train_acc, train_loss\n",
        "\n",
        "  def valid_fn(self, dataloader):\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    lab_list, pred_list = [], []\n",
        "    val_loss = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "\n",
        "      input_ids = data['input_ids'].to(self.device)\n",
        "      masks = data['attention_mask'].to(self.device)\n",
        "      labels = data['label'].to(self.device)\n",
        "\n",
        "      logits = self.model({'input_ids':input_ids, 'attention_mask':masks})\n",
        "      loss = self.loss_fn(logits, labels)\n",
        "      val_loss += loss.item()\n",
        "\n",
        "      lab_list.extend(labels.view(-1).cpu().numpy())\n",
        "      pred_list.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
        "\n",
        "    val_acc = accuracy_score(lab_list, pred_list)\n",
        "    val_loss = val_loss/len(lab_list)\n",
        "\n",
        "    return val_acc, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP3XTLBTKOcA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def run_experiment(fold, cfg, params, save_model=False):\n",
        "\n",
        "  print(params)\n",
        "\n",
        "  #Load model and tokenizer\n",
        "  tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, fast=True)\n",
        "\n",
        "  #pull in df:\n",
        "  df = pd.read_csv(cfg.fold_df_loc)\n",
        "\n",
        "  #Pull out on the split\n",
        "  train_df = df[df.kfold != fold].reset_index(drop=True)\n",
        "  valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
        "\n",
        "  #Create datasets\n",
        "  train_data = WatsonDataset(train_df, tokenizer, cfg.max_len)\n",
        "  valid_data = WatsonDataset(valid_df, tokenizer, cfg.max_len)\n",
        "\n",
        "  #Feed to dataloader\n",
        "  train_loader = DataLoader(train_data, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                num_workers=2, pin_memory=True)\n",
        "  valid_loader = DataLoader(valid_data, batch_size=cfg.batch_size, shuffle=True,\n",
        "                                num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "  #UPDATE CFG:\n",
        "  cfg.num_training_steps = len(train_df)/cfg.batch_size*cfg.max_epochs\n",
        "\n",
        "  cfg.attention_dropout = params['attention_dropout']\n",
        "  cfg.attention_dropout_prob = params['attention_dropout_prob']\n",
        "  cfg.hidden_dropout = params['hidden_dropout']\n",
        "  cfg.hidden_dropout_prob = params['hidden_dropout_prob']\n",
        "  cfg.last_layer_id = params['last_layer_id']\n",
        "  cfg.learning_rate = params['learning_rate']\n",
        "  cfg.layer_norm_eps = params['layer_norm_eps']\n",
        "\n",
        "\n",
        "  #Calculate number of training steps: cfg\n",
        "  model = PooledCustomModel(cfg, pretrained=True)\n",
        "  model = model.to(device)\n",
        "\n",
        "  #Select optimizer\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
        "\n",
        "  #Instantiate for training loop\n",
        "  lrn = Learning(model, device, optimizer, cfg)\n",
        "\n",
        "  best_acc = 0\n",
        "  best_epoch = 0\n",
        "  patience = 1\n",
        "  no_imp = 0\n",
        "\n",
        "  val_acc = []\n",
        "  val_loss = []\n",
        "\n",
        "  for epoch in range(cfg.max_epochs):\n",
        "    train_acc, train_loss = lrn.train_fn(train_loader)\n",
        "    valid_acc, valid_loss = lrn.valid_fn(valid_loader)\n",
        "    \n",
        "    val_acc.append(valid_acc)\n",
        "    val_loss.append(valid_loss)\n",
        "\n",
        "    print(f'validation acc for epoch {epoch}: {valid_acc}')\n",
        "    if valid_acc > best_acc:\n",
        "      best_acc = valid_acc\n",
        "      best_loss = valid_loss\n",
        "      best_epoch = epoch\n",
        "    \n",
        "    else:\n",
        "      no_imp += 1\n",
        "      if no_imp >= patience:\n",
        "        break\n",
        "\n",
        "  #Clean up\n",
        "  model = None\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "  return best_acc, best_loss, best_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMG69A0tBlS"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "  params = {\n",
        "      \"layer_norm_eps\": trial.suggest_float(\"layer_norm_eps\", 1e-8, 1e-4),\n",
        "      \"attention_dropout\": trial.suggest_float(\"attention_dropout\", .1, .5),\n",
        "      \"attention_dropout_prob\": trial.suggest_float(\"attention_dropout_prob\", .1, .5),\n",
        "      \"hidden_dropout\": trial.suggest_float(\"hidden_dropout\", .1, .5),\n",
        "      \"hidden_dropout_prob\": trial.suggest_float(\"hidden_dropout_prob\", .1, .5),\n",
        "      \"last_layer_id\": trial.suggest_int(\"last_layer_id\", -3, -1),\n",
        "      \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-6, 1e-5)\n",
        "      }\n",
        "\n",
        "  all_losses = []\n",
        "\n",
        "  for f_ in range(cfg.n_folds):\n",
        "    best_acc, best_loss, best_epoch = run_experiment(f_, cfg, params)\n",
        "    all_losses.append(best_loss)\n",
        "\n",
        "  return np.mean(all_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wzx8wW4Ftf2D",
        "outputId": "d108d488-fe16-4041-b726-30041571baf9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-03 23:54:46,008]\u001b[0m A new study created in memory with name: no-name-a20b1ba9-638d-4799-b831-710126825a52\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.05240012298570946\n",
            "Training loss after 400 training steps: 0.04565173692069948\n",
            "Training loss after 600 training steps: 0.04373088080358381\n",
            "Training loss after 800 training steps: 0.043266596817120445\n",
            "Training loss after 1000 training steps: 0.042002600682899356\n",
            "Training loss after 1200 training steps: 0.041971399336859276\n",
            "validation acc for epoch 0: 0.9253300330033003\n",
            "Training loss after 200 training steps: 0.026895898627117276\n",
            "Training loss after 400 training steps: 0.029071907158649993\n",
            "Training loss after 600 training steps: 0.028707180234778206\n",
            "Training loss after 800 training steps: 0.028923143827123566\n",
            "Training loss after 1000 training steps: 0.029978673623350913\n",
            "Training loss after 1200 training steps: 0.029940540409346187\n",
            "validation acc for epoch 1: 0.9141914191419142\n",
            "{'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.04817069942713715\n",
            "Training loss after 400 training steps: 0.045784362761187365\n",
            "Training loss after 600 training steps: 0.04511855688140107\n",
            "Training loss after 800 training steps: 0.044264989422663346\n",
            "Training loss after 1000 training steps: 0.04429818662640173\n",
            "Training loss after 1200 training steps: 0.04314874469525724\n",
            "validation acc for epoch 0: 0.9245049504950495\n",
            "Training loss after 200 training steps: 0.026952195964986457\n",
            "Training loss after 400 training steps: 0.029648557736363726\n",
            "Training loss after 600 training steps: 0.030253031167279308\n",
            "Training loss after 800 training steps: 0.03004054684097355\n",
            "Training loss after 1000 training steps: 0.02980239337554667\n",
            "Training loss after 1200 training steps: 0.029824901642326342\n",
            "validation acc for epoch 1: 0.9195544554455446\n",
            "{'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.05079622641555034\n",
            "Training loss after 400 training steps: 0.04760003443050664\n",
            "Training loss after 600 training steps: 0.04540032169199549\n",
            "Training loss after 800 training steps: 0.04531497971562203\n",
            "Training loss after 1000 training steps: 0.043735413489397613\n",
            "Training loss after 1200 training steps: 0.043456723699928264\n",
            "validation acc for epoch 0: 0.9170792079207921\n",
            "Training loss after 200 training steps: 0.027670985938457307\n",
            "Training loss after 400 training steps: 0.028666792707517742\n",
            "Training loss after 600 training steps: 0.029862507641276657\n",
            "Training loss after 800 training steps: 0.0299383040383691\n",
            "Training loss after 1000 training steps: 0.029718215139582754\n",
            "Training loss after 1200 training steps: 0.029609256342834366\n",
            "validation acc for epoch 1: 0.9220297029702971\n",
            "Training loss after 200 training steps: 0.018830382503801958\n",
            "Training loss after 400 training steps: 0.01936304806280532\n",
            "Training loss after 600 training steps: 0.020028074288663145\n",
            "Training loss after 800 training steps: 0.021241786239625072\n",
            "Training loss after 1000 training steps: 0.021492507992428727\n",
            "Training loss after 1200 training steps: 0.02130991858818258\n",
            "validation acc for epoch 2: 0.9166666666666666\n",
            "{'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.05099735717056319\n",
            "Training loss after 400 training steps: 0.04719986035168404\n",
            "Training loss after 600 training steps: 0.04568605317598364\n",
            "Training loss after 800 training steps: 0.0442430685776344\n",
            "Training loss after 1000 training steps: 0.042961562667042014\n",
            "Training loss after 1200 training steps: 0.042895736889719656\n",
            "validation acc for epoch 0: 0.9174917491749175\n",
            "Training loss after 200 training steps: 0.029928183449083007\n",
            "Training loss after 400 training steps: 0.029233838724612726\n",
            "Training loss after 600 training steps: 0.029962817138584796\n",
            "Training loss after 800 training steps: 0.029901252031631884\n",
            "Training loss after 1000 training steps: 0.03028202789957868\n",
            "Training loss after 1200 training steps: 0.03070241453742104\n",
            "validation acc for epoch 1: 0.9154290429042904\n",
            "{'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.05067806611536071\n",
            "Training loss after 400 training steps: 0.047374278649222105\n",
            "Training loss after 600 training steps: 0.04578897397654752\n",
            "Training loss after 800 training steps: 0.04446756251680199\n",
            "Training loss after 1000 training steps: 0.04355558087304234\n",
            "Training loss after 1200 training steps: 0.04296402130945353\n",
            "validation acc for epoch 0: 0.9117161716171617\n",
            "Training loss after 200 training steps: 0.03220942551968619\n",
            "Training loss after 400 training steps: 0.03138928708882304\n",
            "Training loss after 600 training steps: 0.03124208100765827\n",
            "Training loss after 800 training steps: 0.030343366998931743\n",
            "Training loss after 1000 training steps: 0.029799531251890585\n",
            "Training loss after 1200 training steps: 0.029963363035058137\n",
            "validation acc for epoch 1: 0.9104785478547854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 02:37:15,219]\u001b[0m Trial 0 finished with value: 0.029976652201519443 and parameters: {'layer_norm_eps': 3.037430959850435e-05, 'attention_dropout': 0.3813872637346437, 'attention_dropout_prob': 0.32310615290183314, 'hidden_dropout': 0.36452341638266383, 'hidden_dropout_prob': 0.1219189099004625, 'last_layer_id': -3, 'learning_rate': 7.768328378877015e-06}. Best is trial 0 with value: 0.029976652201519443.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.08559100503101945\n",
            "Training loss after 400 training steps: 0.07658629219513387\n",
            "Training loss after 600 training steps: 0.07331825766557207\n",
            "Training loss after 800 training steps: 0.07112116862437688\n",
            "Training loss after 1000 training steps: 0.07014908698294312\n",
            "Training loss after 1200 training steps: 0.0685210584239879\n",
            "validation acc for epoch 0: 0.9088283828382838\n",
            "Training loss after 200 training steps: 0.05596067225327715\n",
            "Training loss after 400 training steps: 0.05639878835412673\n",
            "Training loss after 600 training steps: 0.056488476311787966\n",
            "Training loss after 800 training steps: 0.056380391898564995\n",
            "Training loss after 1000 training steps: 0.0560075896740891\n",
            "Training loss after 1200 training steps: 0.05635783700272441\n",
            "validation acc for epoch 1: 0.905940594059406\n",
            "{'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07954822574742139\n",
            "Training loss after 400 training steps: 0.07450311550172045\n",
            "Training loss after 600 training steps: 0.0714525808347389\n",
            "Training loss after 800 training steps: 0.0709362655109726\n",
            "Training loss after 1000 training steps: 0.07008114586211742\n",
            "Training loss after 1200 training steps: 0.06837617691218233\n",
            "validation acc for epoch 0: 0.9141914191419142\n",
            "Training loss after 200 training steps: 0.05426030838629231\n",
            "Training loss after 400 training steps: 0.05589531335281208\n",
            "Training loss after 600 training steps: 0.05594319388231573\n",
            "Training loss after 800 training steps: 0.056737513845146165\n",
            "Training loss after 1000 training steps: 0.05663938552397303\n",
            "Training loss after 1200 training steps: 0.05616333552345168\n",
            "validation acc for epoch 1: 0.9187293729372937\n",
            "Training loss after 200 training steps: 0.04851053897989914\n",
            "Training loss after 400 training steps: 0.05037237345823087\n",
            "Training loss after 600 training steps: 0.05069331761139135\n",
            "Training loss after 800 training steps: 0.05083288325869944\n",
            "Training loss after 1000 training steps: 0.051326193979475646\n",
            "Training loss after 1200 training steps: 0.051101712766491496\n",
            "validation acc for epoch 2: 0.9117161716171617\n",
            "{'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07635021857451647\n",
            "Training loss after 400 training steps: 0.07035492507740855\n",
            "Training loss after 600 training steps: 0.06836057785976057\n",
            "Training loss after 800 training steps: 0.06688291124359239\n",
            "Training loss after 1000 training steps: 0.06701865461515263\n",
            "Training loss after 1200 training steps: 0.06670776787097565\n",
            "validation acc for epoch 0: 0.9121287128712872\n",
            "Training loss after 200 training steps: 0.05774240296334028\n",
            "Training loss after 400 training steps: 0.05715031647821888\n",
            "Training loss after 600 training steps: 0.05708403179422021\n",
            "Training loss after 800 training steps: 0.057179416125291024\n",
            "Training loss after 1000 training steps: 0.05700548685481772\n",
            "Training loss after 1200 training steps: 0.056281294275152806\n",
            "validation acc for epoch 1: 0.9154290429042904\n",
            "Training loss after 200 training steps: 0.052971671691630036\n",
            "Training loss after 400 training steps: 0.05159858396276831\n",
            "Training loss after 600 training steps: 0.051436082664877175\n",
            "Training loss after 800 training steps: 0.05050478546618251\n",
            "Training loss after 1000 training steps: 0.05048495841003023\n",
            "Training loss after 1200 training steps: 0.050150126012934684\n",
            "validation acc for epoch 2: 0.9038778877887789\n",
            "{'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.08359318607486785\n",
            "Training loss after 400 training steps: 0.07743402203777805\n",
            "Training loss after 600 training steps: 0.07206954896450042\n",
            "Training loss after 800 training steps: 0.07067201011115685\n",
            "Training loss after 1000 training steps: 0.0697199559994042\n",
            "Training loss after 1200 training steps: 0.06886800506617874\n",
            "validation acc for epoch 0: 0.9125412541254125\n",
            "Training loss after 200 training steps: 0.05537958695087582\n",
            "Training loss after 400 training steps: 0.05834010514197871\n",
            "Training loss after 600 training steps: 0.059076927249940736\n",
            "Training loss after 800 training steps: 0.05868692144169472\n",
            "Training loss after 1000 training steps: 0.05777981521934271\n",
            "Training loss after 1200 training steps: 0.057099268607174354\n",
            "validation acc for epoch 1: 0.9001650165016502\n",
            "{'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.08494746808428318\n",
            "Training loss after 400 training steps: 0.07791691597551108\n",
            "Training loss after 600 training steps: 0.07466320392830918\n",
            "Training loss after 800 training steps: 0.07271799257607199\n",
            "Training loss after 1000 training steps: 0.07112711454369128\n",
            "Training loss after 1200 training steps: 0.0699909597208413\n",
            "validation acc for epoch 0: 0.9075907590759076\n",
            "Training loss after 200 training steps: 0.055909174818079915\n",
            "Training loss after 400 training steps: 0.056931074677268045\n",
            "Training loss after 600 training steps: 0.05720066515573611\n",
            "Training loss after 800 training steps: 0.05748404213576577\n",
            "Training loss after 1000 training steps: 0.05669977977615781\n",
            "Training loss after 1200 training steps: 0.05650999823138894\n",
            "validation acc for epoch 1: 0.9141914191419142\n",
            "Training loss after 200 training steps: 0.05172259207814932\n",
            "Training loss after 400 training steps: 0.049817536434857174\n",
            "Training loss after 600 training steps: 0.05051191931900879\n",
            "Training loss after 800 training steps: 0.051169726267107765\n",
            "Training loss after 1000 training steps: 0.049968697236385196\n",
            "Training loss after 1200 training steps: 0.04946560633718036\n",
            "validation acc for epoch 2: 0.9051155115511551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 05:57:26,247]\u001b[0m Trial 1 finished with value: 0.03806280505416221 and parameters: {'layer_norm_eps': 5.882039071882567e-05, 'attention_dropout': 0.22427143924478443, 'attention_dropout_prob': 0.13621338900646426, 'hidden_dropout': 0.2268176631531661, 'hidden_dropout_prob': 0.3483826828318769, 'last_layer_id': -1, 'learning_rate': 8.62331744832147e-06}. Best is trial 0 with value: 0.029976652201519443.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.10890110563486814\n",
            "Training loss after 400 training steps: 0.10283612472005188\n",
            "Training loss after 600 training steps: 0.09859679120903214\n",
            "Training loss after 800 training steps: 0.09703751323511824\n",
            "Training loss after 1000 training steps: 0.09528508994542062\n",
            "Training loss after 1200 training steps: 0.09422580251004548\n",
            "validation acc for epoch 0: 0.908003300330033\n",
            "Training loss after 200 training steps: 0.08042883669491857\n",
            "Training loss after 400 training steps: 0.08113507724367082\n",
            "Training loss after 600 training steps: 0.08194841617097458\n",
            "Training loss after 800 training steps: 0.0811292042862624\n",
            "Training loss after 1000 training steps: 0.08156122187152505\n",
            "Training loss after 1200 training steps: 0.08142828714257727\n",
            "validation acc for epoch 1: 0.9042904290429042\n",
            "{'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.11649884650483727\n",
            "Training loss after 400 training steps: 0.10884339011274279\n",
            "Training loss after 600 training steps: 0.10483689291713139\n",
            "Training loss after 800 training steps: 0.1015253814170137\n",
            "Training loss after 1000 training steps: 0.09897766532190144\n",
            "Training loss after 1200 training steps: 0.09780723388772458\n",
            "validation acc for epoch 0: 0.9067656765676567\n",
            "Training loss after 200 training steps: 0.08630760733503848\n",
            "Training loss after 400 training steps: 0.08577276652446017\n",
            "Training loss after 600 training steps: 0.08584177759941668\n",
            "Training loss after 800 training steps: 0.08389448352041655\n",
            "Training loss after 1000 training steps: 0.0835936710559763\n",
            "Training loss after 1200 training steps: 0.0836248892332272\n",
            "validation acc for epoch 1: 0.893976897689769\n",
            "{'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.11376723995432257\n",
            "Training loss after 400 training steps: 0.10709731337614357\n",
            "Training loss after 600 training steps: 0.10464404935017228\n",
            "Training loss after 800 training steps: 0.10201039898442105\n",
            "Training loss after 1000 training steps: 0.10021954729408025\n",
            "Training loss after 1200 training steps: 0.09843914499506354\n",
            "validation acc for epoch 0: 0.9088283828382838\n",
            "Training loss after 200 training steps: 0.08366468264721334\n",
            "Training loss after 400 training steps: 0.083945600730367\n",
            "Training loss after 600 training steps: 0.08420520004195471\n",
            "Training loss after 800 training steps: 0.08326891044387594\n",
            "Training loss after 1000 training steps: 0.08342228520847857\n",
            "Training loss after 1200 training steps: 0.08404720510200908\n",
            "validation acc for epoch 1: 0.9071782178217822\n",
            "{'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.1068546899035573\n",
            "Training loss after 400 training steps: 0.10060257314704359\n",
            "Training loss after 600 training steps: 0.09867275383944313\n",
            "Training loss after 800 training steps: 0.09671104257460683\n",
            "Training loss after 1000 training steps: 0.0947219348885119\n",
            "Training loss after 1200 training steps: 0.0940408895087118\n",
            "validation acc for epoch 0: 0.9026402640264026\n",
            "Training loss after 200 training steps: 0.08383742458187043\n",
            "Training loss after 400 training steps: 0.08383683062624186\n",
            "Training loss after 600 training steps: 0.08382696219409505\n",
            "Training loss after 800 training steps: 0.08348571942886338\n",
            "Training loss after 1000 training steps: 0.08286230299342423\n",
            "Training loss after 1200 training steps: 0.08247606230666861\n",
            "validation acc for epoch 1: 0.9018151815181518\n",
            "{'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.11614036772400141\n",
            "Training loss after 400 training steps: 0.10803137256763876\n",
            "Training loss after 600 training steps: 0.10381411035234729\n",
            "Training loss after 800 training steps: 0.10097948839655146\n",
            "Training loss after 1000 training steps: 0.10004953932017088\n",
            "Training loss after 1200 training steps: 0.09830295187265922\n",
            "validation acc for epoch 0: 0.9055280528052805\n",
            "Training loss after 200 training steps: 0.08777929798699916\n",
            "Training loss after 400 training steps: 0.08598630333784968\n",
            "Training loss after 600 training steps: 0.08581925698866447\n",
            "Training loss after 800 training steps: 0.08489054083125666\n",
            "Training loss after 1000 training steps: 0.08408131841570139\n",
            "Training loss after 1200 training steps: 0.08340118273471793\n",
            "validation acc for epoch 1: 0.9084158415841584\n",
            "Training loss after 200 training steps: 0.07769494107458741\n",
            "Training loss after 400 training steps: 0.0783567357226275\n",
            "Training loss after 600 training steps: 0.07844763101544232\n",
            "Training loss after 800 training steps: 0.07916960088652558\n",
            "Training loss after 1000 training steps: 0.07815342628862709\n",
            "Training loss after 1200 training steps: 0.07782309167164689\n",
            "validation acc for epoch 2: 0.9022277227722773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 08:47:01,311]\u001b[0m Trial 2 finished with value: 0.040441166894474266 and parameters: {'layer_norm_eps': 2.5379247386742868e-05, 'attention_dropout': 0.1705550338561245, 'attention_dropout_prob': 0.30373598800048485, 'hidden_dropout': 0.14153848628076127, 'hidden_dropout_prob': 0.3703954684927171, 'last_layer_id': -1, 'learning_rate': 8.344314563988124e-06}. Best is trial 0 with value: 0.029976652201519443.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07429959315340966\n",
            "Training loss after 400 training steps: 0.07278370465384797\n",
            "Training loss after 600 training steps: 0.06999499885210146\n",
            "Training loss after 800 training steps: 0.0685775124450447\n",
            "Training loss after 1000 training steps: 0.0667332432908006\n",
            "Training loss after 1200 training steps: 0.06652898608900917\n",
            "validation acc for epoch 0: 0.9187293729372937\n",
            "Training loss after 200 training steps: 0.05205475815921091\n",
            "Training loss after 400 training steps: 0.05220821281604003\n",
            "Training loss after 600 training steps: 0.0533153486410932\n",
            "Training loss after 800 training steps: 0.054234366747550666\n",
            "Training loss after 1000 training steps: 0.05397410627175123\n",
            "Training loss after 1200 training steps: 0.05335689441883005\n",
            "validation acc for epoch 1: 0.9228547854785478\n",
            "Training loss after 200 training steps: 0.044418180956854486\n",
            "Training loss after 400 training steps: 0.046215515568328557\n",
            "Training loss after 600 training steps: 0.04735559452189288\n",
            "Training loss after 800 training steps: 0.047990422703587686\n",
            "Training loss after 1000 training steps: 0.04770941188500728\n",
            "Training loss after 1200 training steps: 0.0476121142222352\n",
            "validation acc for epoch 2: 0.9162541254125413\n",
            "{'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07564115227200091\n",
            "Training loss after 400 training steps: 0.07011958425631747\n",
            "Training loss after 600 training steps: 0.06791665824207788\n",
            "Training loss after 800 training steps: 0.06636484190297778\n",
            "Training loss after 1000 training steps: 0.06563937350315974\n",
            "Training loss after 1200 training steps: 0.0641645145715059\n",
            "validation acc for epoch 0: 0.9174917491749175\n",
            "Training loss after 200 training steps: 0.052610508078942075\n",
            "Training loss after 400 training steps: 0.05427556356822606\n",
            "Training loss after 600 training steps: 0.053700804344456024\n",
            "Training loss after 800 training steps: 0.05336618671281031\n",
            "Training loss after 1000 training steps: 0.053718779475195336\n",
            "Training loss after 1200 training steps: 0.05284469097435552\n",
            "validation acc for epoch 1: 0.9166666666666666\n",
            "{'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07309150097891688\n",
            "Training loss after 400 training steps: 0.07066554232733324\n",
            "Training loss after 600 training steps: 0.0681154410704039\n",
            "Training loss after 800 training steps: 0.06703232609957922\n",
            "Training loss after 1000 training steps: 0.06646822433406487\n",
            "Training loss after 1200 training steps: 0.06514925836391437\n",
            "validation acc for epoch 0: 0.9137788778877888\n",
            "Training loss after 200 training steps: 0.0492925199912861\n",
            "Training loss after 400 training steps: 0.05378982972819358\n",
            "Training loss after 600 training steps: 0.053714942826579015\n",
            "Training loss after 800 training steps: 0.0526278467332304\n",
            "Training loss after 1000 training steps: 0.052630616333452056\n",
            "Training loss after 1200 training steps: 0.05275370609024928\n",
            "validation acc for epoch 1: 0.9158415841584159\n",
            "Training loss after 200 training steps: 0.04739596599247307\n",
            "Training loss after 400 training steps: 0.046041282893856984\n",
            "Training loss after 600 training steps: 0.04739649157505482\n",
            "Training loss after 800 training steps: 0.04675786856736522\n",
            "Training loss after 1000 training steps: 0.047627502266084774\n",
            "Training loss after 1200 training steps: 0.04802597216078235\n",
            "validation acc for epoch 2: 0.9067656765676567\n",
            "{'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.08278561639832333\n",
            "Training loss after 400 training steps: 0.07650208684732206\n",
            "Training loss after 600 training steps: 0.07313729486195371\n",
            "Training loss after 800 training steps: 0.07052558352064807\n",
            "Training loss after 1000 training steps: 0.06849612043285742\n",
            "Training loss after 1200 training steps: 0.06672013149325115\n",
            "validation acc for epoch 0: 0.9084158415841584\n",
            "Training loss after 200 training steps: 0.056398420054465534\n",
            "Training loss after 400 training steps: 0.054674887547735126\n",
            "Training loss after 600 training steps: 0.055236247105058284\n",
            "Training loss after 800 training steps: 0.05528045467712218\n",
            "Training loss after 1000 training steps: 0.05515342596708797\n",
            "Training loss after 1200 training steps: 0.05510562890539101\n",
            "validation acc for epoch 1: 0.9108910891089109\n",
            "Training loss after 200 training steps: 0.047137490534223614\n",
            "Training loss after 400 training steps: 0.048053082898259165\n",
            "Training loss after 600 training steps: 0.048929582845109204\n",
            "Training loss after 800 training steps: 0.04846816820616368\n",
            "Training loss after 1000 training steps: 0.047855728322872895\n",
            "Training loss after 1200 training steps: 0.047799848023472195\n",
            "validation acc for epoch 2: 0.9051155115511551\n",
            "{'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.07782729270868004\n",
            "Training loss after 400 training steps: 0.07393110897392034\n",
            "Training loss after 600 training steps: 0.07120606805197895\n",
            "Training loss after 800 training steps: 0.06866618008527439\n",
            "Training loss after 1000 training steps: 0.06713153596455232\n",
            "Training loss after 1200 training steps: 0.06578194921952672\n",
            "validation acc for epoch 0: 0.9154290429042904\n",
            "Training loss after 200 training steps: 0.056994543364271524\n",
            "Training loss after 400 training steps: 0.054349488178268075\n",
            "Training loss after 600 training steps: 0.05497161320100228\n",
            "Training loss after 800 training steps: 0.054797968014609066\n",
            "Training loss after 1000 training steps: 0.05447969533340074\n",
            "Training loss after 1200 training steps: 0.05416261882346589\n",
            "validation acc for epoch 1: 0.9063531353135313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 12:07:19,206]\u001b[0m Trial 3 finished with value: 0.03681181979878758 and parameters: {'layer_norm_eps': 6.375650973803733e-05, 'attention_dropout': 0.42825411182346473, 'attention_dropout_prob': 0.2879274268206374, 'hidden_dropout': 0.1840338572204422, 'hidden_dropout_prob': 0.293527886691375, 'last_layer_id': -1, 'learning_rate': 6.8743790886822445e-06}. Best is trial 0 with value: 0.029976652201519443.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.13902735441923142\n",
            "Training loss after 400 training steps: 0.13827310448512434\n",
            "Training loss after 600 training steps: 0.1381833571071426\n",
            "Training loss after 800 training steps: 0.1379712153505534\n",
            "Training loss after 1000 training steps: 0.1378848413825035\n",
            "Training loss after 1200 training steps: 0.13794133626545468\n",
            "validation acc for epoch 0: 0.643976897689769\n",
            "Training loss after 200 training steps: 0.13774847023189069\n",
            "Training loss after 400 training steps: 0.13771587606519461\n",
            "Training loss after 600 training steps: 0.13779234364628792\n",
            "Training loss after 800 training steps: 0.1379703204892576\n",
            "Training loss after 1000 training steps: 0.1379681919515133\n",
            "Training loss after 1200 training steps: 0.13789924360811712\n",
            "validation acc for epoch 1: 0.7186468646864687\n",
            "Training loss after 200 training steps: 0.1380418971180916\n",
            "Training loss after 400 training steps: 0.1380064807087183\n",
            "Training loss after 600 training steps: 0.13793427004168432\n",
            "Training loss after 800 training steps: 0.13786125615239142\n",
            "Training loss after 1000 training steps: 0.13784715843200684\n",
            "Training loss after 1200 training steps: 0.1378522368768851\n",
            "validation acc for epoch 2: 0.6344884488448845\n",
            "{'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.13930585004389287\n",
            "Training loss after 400 training steps: 0.13903479700908064\n",
            "Training loss after 600 training steps: 0.13877135891467332\n",
            "Training loss after 800 training steps: 0.1385909024439752\n",
            "Training loss after 1000 training steps: 0.13847685950994493\n",
            "Training loss after 1200 training steps: 0.13843224133054416\n",
            "validation acc for epoch 0: 0.4414191419141914\n",
            "Training loss after 200 training steps: 0.13772949520498515\n",
            "Training loss after 400 training steps: 0.13798018224537373\n",
            "Training loss after 600 training steps: 0.1379326473424832\n",
            "Training loss after 800 training steps: 0.13799613348208367\n",
            "Training loss after 1000 training steps: 0.1380549480021\n",
            "Training loss after 1200 training steps: 0.13804543318847814\n",
            "validation acc for epoch 1: 0.4438943894389439\n",
            "Training loss after 200 training steps: 0.1381836212798953\n",
            "Training loss after 400 training steps: 0.13816043535247446\n",
            "Training loss after 600 training steps: 0.13811050026367108\n",
            "Training loss after 800 training steps: 0.1380031253490597\n",
            "Training loss after 1000 training steps: 0.13788622745126486\n",
            "Training loss after 1200 training steps: 0.1378328324171404\n",
            "validation acc for epoch 2: 0.42574257425742573\n",
            "{'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.13984859071671962\n",
            "Training loss after 400 training steps: 0.1393238328769803\n",
            "Training loss after 600 training steps: 0.13901053259770074\n",
            "Training loss after 800 training steps: 0.13890216570347547\n",
            "Training loss after 1000 training steps: 0.1387344343960285\n",
            "Training loss after 1200 training steps: 0.13859936326121292\n",
            "validation acc for epoch 0: 0.5420792079207921\n",
            "Training loss after 200 training steps: 0.13763473358005285\n",
            "Training loss after 400 training steps: 0.1380191671475768\n",
            "Training loss after 600 training steps: 0.13802861080815396\n",
            "Training loss after 800 training steps: 0.13800246758386492\n",
            "Training loss after 1000 training steps: 0.13799465897679328\n",
            "Training loss after 1200 training steps: 0.13793404676641027\n",
            "validation acc for epoch 1: 0.5057755775577558\n",
            "{'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.1397576016932726\n",
            "Training loss after 400 training steps: 0.1393242248147726\n",
            "Training loss after 600 training steps: 0.13909682261447112\n",
            "Training loss after 800 training steps: 0.1389015629608184\n",
            "Training loss after 1000 training steps: 0.13888612492382527\n",
            "Training loss after 1200 training steps: 0.13877773318439723\n",
            "validation acc for epoch 0: 0.32755775577557755\n",
            "Training loss after 200 training steps: 0.13835772942751645\n",
            "Training loss after 400 training steps: 0.13845769930630922\n",
            "Training loss after 600 training steps: 0.13860088373223942\n",
            "Training loss after 800 training steps: 0.13855194711126387\n",
            "Training loss after 1000 training steps: 0.13850034563988448\n",
            "Training loss after 1200 training steps: 0.13846086974566182\n",
            "validation acc for epoch 1: 0.408003300330033\n",
            "Training loss after 200 training steps: 0.13843766309320926\n",
            "Training loss after 400 training steps: 0.1382936557754874\n",
            "Training loss after 600 training steps: 0.1382338222116232\n",
            "Training loss after 800 training steps: 0.1381870049983263\n",
            "Training loss after 1000 training steps: 0.13818864293396474\n",
            "Training loss after 1200 training steps: 0.13821477035681407\n",
            "validation acc for epoch 2: 0.5523927392739274\n",
            "{'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.13904252126812935\n",
            "Training loss after 400 training steps: 0.13887168934568764\n",
            "Training loss after 600 training steps: 0.1386193286255002\n",
            "Training loss after 800 training steps: 0.13833133475854992\n",
            "Training loss after 1000 training steps: 0.13823990152031182\n",
            "Training loss after 1200 training steps: 0.13813990458225212\n",
            "validation acc for epoch 0: 0.36303630363036304\n",
            "Training loss after 200 training steps: 0.13807430148124694\n",
            "Training loss after 400 training steps: 0.13808841494843363\n",
            "Training loss after 600 training steps: 0.13784191321581601\n",
            "Training loss after 800 training steps: 0.13773713489063083\n",
            "Training loss after 1000 training steps: 0.13777366145700215\n",
            "Training loss after 1200 training steps: 0.13782341156775751\n",
            "validation acc for epoch 1: 0.44636963696369636\n",
            "Training loss after 200 training steps: 0.13785799600183965\n",
            "Training loss after 400 training steps: 0.1378373960033059\n",
            "Training loss after 600 training steps: 0.13790249121685824\n",
            "Training loss after 800 training steps: 0.13793682046234607\n",
            "Training loss after 1000 training steps: 0.1379462288469076\n",
            "Training loss after 1200 training steps: 0.13783209388454756\n",
            "validation acc for epoch 2: 0.4278052805280528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 15:33:37,701]\u001b[0m Trial 4 finished with value: 0.12788211210037614 and parameters: {'layer_norm_eps': 4.050461696302612e-05, 'attention_dropout': 0.18295876771253888, 'attention_dropout_prob': 0.2632682319084323, 'hidden_dropout': 0.18742515587640418, 'hidden_dropout_prob': 0.49722016841474825, 'last_layer_id': -3, 'learning_rate': 5.593318459545958e-06}. Best is trial 0 with value: 0.029976652201519443.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.05079983341274783\n",
            "Training loss after 400 training steps: 0.04764340027642902\n",
            "Training loss after 600 training steps: 0.044964443278149704\n",
            "Training loss after 800 training steps: 0.045433155931386866\n",
            "Training loss after 1000 training steps: 0.0449886674520094\n",
            "Training loss after 1200 training steps: 0.0444338748556523\n",
            "validation acc for epoch 0: 0.9183168316831684\n",
            "Training loss after 200 training steps: 0.02952438687090762\n",
            "Training loss after 400 training steps: 0.03308701814909\n",
            "Training loss after 600 training steps: 0.032380030226292246\n",
            "Training loss after 800 training steps: 0.032162701544511944\n",
            "Training loss after 1000 training steps: 0.03192894394992618\n",
            "Training loss after 1200 training steps: 0.03199906297018364\n",
            "validation acc for epoch 1: 0.9179042904290429\n",
            "{'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.052728720334125685\n",
            "Training loss after 400 training steps: 0.04861236622440629\n",
            "Training loss after 600 training steps: 0.04701065598987043\n",
            "Training loss after 800 training steps: 0.046402938847459156\n",
            "Training loss after 1000 training steps: 0.045251844686979896\n",
            "Training loss after 1200 training steps: 0.04428677329099931\n",
            "validation acc for epoch 0: 0.9125412541254125\n",
            "Training loss after 200 training steps: 0.02906024880008772\n",
            "Training loss after 400 training steps: 0.03066111563704908\n",
            "Training loss after 600 training steps: 0.03050659780468171\n",
            "Training loss after 800 training steps: 0.03134957229915017\n",
            "Training loss after 1000 training steps: 0.03139693514123792\n",
            "Training loss after 1200 training steps: 0.031755023492393473\n",
            "validation acc for epoch 1: 0.9224422442244224\n",
            "Training loss after 200 training steps: 0.022858807262746268\n",
            "Training loss after 400 training steps: 0.023286310807416156\n",
            "Training loss after 600 training steps: 0.022805435769963273\n",
            "Training loss after 800 training steps: 0.023328359997867663\n",
            "Training loss after 1000 training steps: 0.023223963626718615\n",
            "Training loss after 1200 training steps: 0.02374183960685817\n",
            "validation acc for epoch 2: 0.9113036303630363\n",
            "{'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.04955141397891566\n",
            "Training loss after 400 training steps: 0.04755690866441\n",
            "Training loss after 600 training steps: 0.04702706533134915\n",
            "Training loss after 800 training steps: 0.04648580389388371\n",
            "Training loss after 1000 training steps: 0.04438525993691292\n",
            "Training loss after 1200 training steps: 0.043296874879888494\n",
            "validation acc for epoch 0: 0.9212046204620462\n",
            "Training loss after 200 training steps: 0.03188735399307916\n",
            "Training loss after 400 training steps: 0.0321890661182988\n",
            "Training loss after 600 training steps: 0.03114237418281846\n",
            "Training loss after 800 training steps: 0.03132565644860733\n",
            "Training loss after 1000 training steps: 0.03106579195521772\n",
            "Training loss after 1200 training steps: 0.03073744807940481\n",
            "validation acc for epoch 1: 0.9096534653465347\n",
            "{'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.04821391175268218\n",
            "Training loss after 400 training steps: 0.04553309990442358\n",
            "Training loss after 600 training steps: 0.043549319713298854\n",
            "Training loss after 800 training steps: 0.04377233237057226\n",
            "Training loss after 1000 training steps: 0.04325152118253754\n",
            "Training loss after 1200 training steps: 0.04342820486902686\n",
            "validation acc for epoch 0: 0.9129537953795379\n",
            "Training loss after 200 training steps: 0.034929687596159054\n",
            "Training loss after 400 training steps: 0.034985626788984515\n",
            "Training loss after 600 training steps: 0.033559558923298025\n",
            "Training loss after 800 training steps: 0.03365001018646581\n",
            "Training loss after 1000 training steps: 0.032652427372697274\n",
            "Training loss after 1200 training steps: 0.03269215205097377\n",
            "validation acc for epoch 1: 0.9146039603960396\n",
            "Training loss after 200 training steps: 0.022927603650896345\n",
            "Training loss after 400 training steps: 0.025012309740995987\n",
            "Training loss after 600 training steps: 0.024849617884271234\n",
            "Training loss after 800 training steps: 0.025041006544088303\n",
            "Training loss after 1000 training steps: 0.025219906620593976\n",
            "Training loss after 1200 training steps: 0.024655884150585432\n",
            "validation acc for epoch 2: 0.905940594059406\n",
            "{'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.0479722162510734\n",
            "Training loss after 400 training steps: 0.04528508842835435\n",
            "Training loss after 600 training steps: 0.043339842556742954\n",
            "Training loss after 800 training steps: 0.043260445270425406\n",
            "Training loss after 1000 training steps: 0.042863275597221216\n",
            "Training loss after 1200 training steps: 0.042413191689314164\n",
            "validation acc for epoch 0: 0.9191419141914191\n",
            "Training loss after 200 training steps: 0.02587222912290599\n",
            "Training loss after 400 training steps: 0.02778377853668644\n",
            "Training loss after 600 training steps: 0.029400093770139694\n",
            "Training loss after 800 training steps: 0.029202694101913947\n",
            "Training loss after 1000 training steps: 0.03010815195000032\n",
            "Training loss after 1200 training steps: 0.03083967467736026\n",
            "validation acc for epoch 1: 0.9051155115511551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 18:38:36,426]\u001b[0m Trial 5 finished with value: 0.02980479276846639 and parameters: {'layer_norm_eps': 4.4066207025251415e-05, 'attention_dropout': 0.21855051048430157, 'attention_dropout_prob': 0.33156994148908525, 'hidden_dropout': 0.11616503165025507, 'hidden_dropout_prob': 0.11487138562729991, 'last_layer_id': -1, 'learning_rate': 9.281695120611431e-06}. Best is trial 5 with value: 0.02980479276846639.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.09742305137217044\n",
            "Training loss after 400 training steps: 0.09168182430323213\n",
            "Training loss after 600 training steps: 0.08773213522819182\n",
            "Training loss after 800 training steps: 0.08553662926191465\n",
            "Training loss after 1000 training steps: 0.08394123253691942\n",
            "Training loss after 1200 training steps: 0.08241845713462681\n",
            "validation acc for epoch 0: 0.9154290429042904\n",
            "Training loss after 200 training steps: 0.06788180741015822\n",
            "Training loss after 400 training steps: 0.07091365102678537\n",
            "Training loss after 600 training steps: 0.07115379504859448\n",
            "Training loss after 800 training steps: 0.07180568415438757\n",
            "Training loss after 1000 training steps: 0.07139445341564715\n",
            "Training loss after 1200 training steps: 0.07061976366249534\n",
            "validation acc for epoch 1: 0.9121287128712872\n",
            "{'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.10210991274565458\n",
            "Training loss after 400 training steps: 0.09413309387397022\n",
            "Training loss after 600 training steps: 0.09102630540418129\n",
            "Training loss after 800 training steps: 0.08826271231053397\n",
            "Training loss after 1000 training steps: 0.0855296936975792\n",
            "Training loss after 1200 training steps: 0.08377986823751901\n",
            "validation acc for epoch 0: 0.9141914191419142\n",
            "Training loss after 200 training steps: 0.07211788523010909\n",
            "Training loss after 400 training steps: 0.06968048721784725\n",
            "Training loss after 600 training steps: 0.06905641495560606\n",
            "Training loss after 800 training steps: 0.06913015470607206\n",
            "Training loss after 1000 training steps: 0.06828059875033796\n",
            "Training loss after 1200 training steps: 0.0675842479915203\n",
            "validation acc for epoch 1: 0.9100660066006601\n",
            "{'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.09916034602560103\n",
            "Training loss after 400 training steps: 0.09317873511929065\n",
            "Training loss after 600 training steps: 0.08753768481624623\n",
            "Training loss after 800 training steps: 0.08418421996524557\n",
            "Training loss after 1000 training steps: 0.08278124459460377\n",
            "Training loss after 1200 training steps: 0.08138871290100118\n",
            "validation acc for epoch 0: 0.9113036303630363\n",
            "Training loss after 200 training steps: 0.06813662331085651\n",
            "Training loss after 400 training steps: 0.06975432363105938\n",
            "Training loss after 600 training steps: 0.06852088726358488\n",
            "Training loss after 800 training steps: 0.06892001709609757\n",
            "Training loss after 1000 training steps: 0.06822012529755012\n",
            "Training loss after 1200 training steps: 0.06797260260325856\n",
            "validation acc for epoch 1: 0.9055280528052805\n",
            "{'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.10840583121404052\n",
            "Training loss after 400 training steps: 0.09940199424512684\n",
            "Training loss after 600 training steps: 0.09284369695621232\n",
            "Training loss after 800 training steps: 0.08910035797045567\n",
            "Training loss after 1000 training steps: 0.08651138073019683\n",
            "Training loss after 1200 training steps: 0.08511691431282088\n",
            "validation acc for epoch 0: 0.9018151815181518\n",
            "Training loss after 200 training steps: 0.06828231587540358\n",
            "Training loss after 400 training steps: 0.07107375487685204\n",
            "Training loss after 600 training steps: 0.07077041277661919\n",
            "Training loss after 800 training steps: 0.071349931077566\n",
            "Training loss after 1000 training steps: 0.07006483095698059\n",
            "Training loss after 1200 training steps: 0.06919574194781793\n",
            "validation acc for epoch 1: 0.898102310231023\n",
            "{'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.09948994142003358\n",
            "Training loss after 400 training steps: 0.09061214630492032\n",
            "Training loss after 600 training steps: 0.08815529856830835\n",
            "Training loss after 800 training steps: 0.08525891138473525\n",
            "Training loss after 1000 training steps: 0.08347475728206337\n",
            "Training loss after 1200 training steps: 0.08117030708429714\n",
            "validation acc for epoch 0: 0.9055280528052805\n",
            "Training loss after 200 training steps: 0.06649654109496624\n",
            "Training loss after 400 training steps: 0.06938131520524621\n",
            "Training loss after 600 training steps: 0.06801595942505324\n",
            "Training loss after 800 training steps: 0.06712832045159303\n",
            "Training loss after 1000 training steps: 0.06692503215558827\n",
            "Training loss after 1200 training steps: 0.06655393493594602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-10-04 21:09:52,297]\u001b[0m Trial 6 finished with value: 0.03516959551634981 and parameters: {'layer_norm_eps': 8.062777030495783e-05, 'attention_dropout': 0.4855381495921697, 'attention_dropout_prob': 0.4467636640833267, 'hidden_dropout': 0.22779719425671316, 'hidden_dropout_prob': 0.2652048337141848, 'last_layer_id': -2, 'learning_rate': 7.147419945425295e-06}. Best is trial 5 with value: 0.02980479276846639.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation acc for epoch 1: 0.9042904290429042\n",
            "{'layer_norm_eps': 7.608310991151871e-05, 'attention_dropout': 0.2808706151117537, 'attention_dropout_prob': 0.13711302895034547, 'hidden_dropout': 0.33438628848911534, 'hidden_dropout_prob': 0.41812717114401143, 'last_layer_id': -1, 'learning_rate': 5.162688235743372e-06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss after 200 training steps: 0.11043811371549964\n",
            "Training loss after 400 training steps: 0.10311910153366625\n",
            "Training loss after 600 training steps: 0.0994136209289233\n",
            "Training loss after 800 training steps: 0.09678489528829232\n",
            "Training loss after 1000 training steps: 0.09503364639729261\n",
            "Training loss after 1200 training steps: 0.09335213714589675\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction = 'minimize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "print(\"Best Trial:\")\n",
        "trial_ = study.best_trial\n",
        "print(trial_.values)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}